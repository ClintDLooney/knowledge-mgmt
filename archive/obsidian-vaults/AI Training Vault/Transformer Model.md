[[Self-Attention Transformer Model]]

* Definition
	* Performs [[Encoding]] on large blocks of dada (generally text) to understand the purpose of sentences by assigning numerical values to word order. It tries to find the purpose of the sentence, rather than the Platonic form of an image. 
* Application
	* Used in transformer-based chat systems like [[ChatGPT]], focusing on understanding context and generating coherent text.
	* 


First there is the compute required to run large language models (LLMs). Amazon has two specialized chips for this very purpose. Details are as follows:

- AWS Inferentia and AWS Trainium are purpose-built ML accelerators that AWS designed from the ground up.
- The first-generation of AWS Inferentia delivers significant performance and cost-savings benefits for deploying smaller models. AWS Trainium and AWS Inferentia2 are built for training and deploying ultra-large generative AI models with hundreds of billions of parameters.
//TODO Define and learn.  Is an LLM a foundation model?
These systems learn from huge amounts of internet text so they can predict the next word from input to generate output from prompts. They generate new text by repeatedly guessing the next word they should output. 
Generated by using [[Supervised Machine Learning]] to repeatedly to repeatedly predict the missing next word. 
An LLM is what you get when you train an AI system on a massive text data set, hundreds of billions or perhaps trillions of words. 

The basics: An LLM is a technology that 
* Use a shitload of data.
* To learn via supervised learning. 
* To predict the next word. 

For good LLM creation, you want
* As much data as you can get. 
* To train the largest neural net you can. 
